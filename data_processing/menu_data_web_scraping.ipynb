{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developled on Python version 3.11.4\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "today=date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that parses through the nutritonal data, and returns a dictionary \n",
    "\n",
    "def nutrition_info_parsing(text):\n",
    "    nutrition_lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    nutrient_dict = {}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(nutrition_lines):\n",
    "        line = nutrition_lines[i]\n",
    "\n",
    "        if line == 'Calories':\n",
    "            nutrient_dict['Calories'] = float(nutrition_lines[i+1])\n",
    "            i += 2  # Increment by 2 to jump to the next component\n",
    "\n",
    "        elif any(word in line for word in ['Fat' ,'Cholesterol','Includes', 'Sugars', 'Sodium', 'Carbohydrates', 'Fiber', 'Protein', 'Vitamin D', 'Calcium', 'Iron', 'Potassium']):\n",
    "            nutrient = line.split()\n",
    "            \n",
    "            # Check if the next line contains a value (like '8g', '25mg', etc.)\n",
    "            if i+1 < len(nutrition_lines) and any(val in nutrition_lines[i+1] for val in ['g', 'mg', 'mcg']):\n",
    "\n",
    "                # Use the nutrient as key and the next line as value\n",
    "                nutrient_name = ' '.join(nutrient[:-1])  # Exclude the value (like '8g') from the nutrient name\n",
    "                unit = ''.join([char for char in nutrient[-1] if not char.isdigit() and char != '.'])\n",
    "                nutrient_key = f\"{nutrient_name} ({unit})\"\n",
    "                nutrient_value = float(''.join([char for char in nutrient[-1] if char.isdigit() or char == '.']))\n",
    "\n",
    "                if '<' in nutrient_name:\n",
    "                    nutrient_name = nutrient_name.replace('<', '').strip()\n",
    "                    nutrient_key = f\"{nutrient_name} ({unit})\"\n",
    "\n",
    "                nutrient_dict[nutrient_key] = nutrient_value\n",
    "                \n",
    "                i += 2  # Increment by 2 to jump to the next component\n",
    "\n",
    "            else:\n",
    "                i += 1  # No expected value on the next line, move on\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    return nutrient_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next series of function uses Selenium to automate the webscraping processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_menu_section_links(driver, base_url=\"https://www.tacobell.com\", menu_endpoint=\"/food\", store_location = \"?store=038911#\"):\n",
    "    driver.get(base_url+menu_endpoint)\n",
    "\n",
    "    cites_allowed_WS = [\n",
    "    \"/food/tacos\",\n",
    "    \"/food/burritos\",\n",
    "    \"/food/quesadillas\",\n",
    "    \"/food/nachos\",\n",
    "    \"/food/sides-sweets\",\n",
    "    \"/food/drinks\",\n",
    "    \"/food/power-menu\",\n",
    "    \"/food/vegetarian\",\n",
    "    \"/food/breakfast\",\n",
    "    \"/food/specialties\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # Look into https://www.tacobell.com/sitemap.xml a little bit more.\n",
    "\n",
    "    sleep(5)  # Wait 5 seconds before searching for the element\n",
    "\n",
    "    # Using XPath to locate the main parent div that contains all the links\n",
    "    # element = driver.find_element(By.XPATH, '//div[contains(@class, \"styles_menu-tiles__1JTJ3\")]')\n",
    "    element = driver.find_element(By.XPATH, '//div[contains(@class, \"styles_menu-grid__9lRvR\")]')\n",
    "\n",
    "\n",
    "    # Parse the content using BeautifulSoup\n",
    "    soup = BeautifulSoup(element.get_attribute('outerHTML'), 'html.parser')\n",
    "\n",
    "    # Extract all the links and their href values\n",
    "    links = [a['href'] for a in soup.find_all('a') if a.has_attr('href')]\n",
    "\n",
    "    allowed_links = [link for link in links if link in cites_allowed_WS]\n",
    "    \n",
    "    section_links = [base_url + link + store_location for link in allowed_links]\n",
    "\n",
    "\n",
    "    return section_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pulling_data(driver, store_location=\"?store=038911#\", base_url=\"https://www.tacobell.com\"):\n",
    "    menu_section_links = get_menu_section_links(driver)\n",
    "\n",
    "    menu_data = []\n",
    "    for f in menu_section_links:\n",
    "        driver.get(f)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        heading = soup.find('h1')\n",
    "        \n",
    "        links = [a['href'] for a in soup.find_all('a', class_='styles_product-title__6KCyw')]\n",
    "        \n",
    "        full_links = [base_url + link + store_location+\"#\" if not link.endswith('1') else base_url+link+\"#\" for link in links]\n",
    "\n",
    "        for item in full_links:\n",
    "            driver.get(item)\n",
    "\n",
    "            subpage_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            header = subpage_soup.find_all('h1')\n",
    "            item_name = [i.text for i in header if len(i.text) !=0]\n",
    "\n",
    "            price = subpage_soup.find('span', class_='styles_price__3-xtw')\n",
    "            # print(f\"Menu Section: {heading.text}, Item name: {item_name[0]}, Price: {price.text}\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Check if the \"Nutrition Info\" link exists on the webpage\n",
    "                nutrition_link = driver.find_element(By.LINK_TEXT, \"Nutrition Info\")\n",
    "                nutrition_link.click()\n",
    "                sleep(3)\n",
    "                \n",
    "                driver.switch_to.frame(driver.find_element(By.XPATH, '//iframe[contains(@src, \"nutritionix.com/label/popup/item/\")]'))\n",
    "\n",
    "                \n",
    "                # Grabbing nutri info\n",
    "                nutrition_info = driver.find_element(By.CLASS_NAME, 'nf')\n",
    "                nutrition_info_txt = nutrition_info.text\n",
    "\n",
    "                # print(f\"Nutrition info for {item_name[0]} grabbed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Grabbing allergen info; still need to find a way to clean this text\n",
    "                # allergen_info = driver.find_element(By.CLASS_NAME, \"allergenInfo\")\n",
    "                # allergen_info_text = allergen_info.text\n",
    "\n",
    "            \n",
    "\n",
    "                # Append the data to the taco_data list\n",
    "                menu_data.append({\n",
    "                    'item_name': item_name[0],\n",
    "                    'price': float(price.text[1:]),\n",
    "                    'menu_section': heading.text,\n",
    "                    **nutrition_info_parsing(nutrition_info_txt)\n",
    "                })\n",
    "\n",
    "                print(f\"All of the {item_name[0]} data added to the master dataset\")\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            except NoSuchElementException:  # Element not found\n",
    "                continue  # Go to the next item in the loop\n",
    "\n",
    "        print(f\"\\nThe {heading.text} section has been sucessfully pulled\")\n",
    "\n",
    "\n",
    "    print(\"\\nAll individual items from Taco Bell's menu have been acquired\")\n",
    "\n",
    "\n",
    "    return menu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "menu_data = pulling_data(driver)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_data_df = pd.DataFrame(menu_data)\n",
    "menu_data_df.info\n",
    "print(menu_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_data_df.to_csv(f\"../data/uncleaned_taco_bell_menu_items{today}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "def clean_taco_bell_menu(input_csv_path, output_csv_path=None):\n",
    "    # Step 1: Set options\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    # Step 2: Read the data\n",
    "    menu_data_df = pd.read_csv(input_csv_path, index_col=0)\n",
    "    \n",
    "    # Step 3: Fill missing values and adjust columns\n",
    "    menu_data_df.fillna(0, inplace=True)\n",
    "    menu_data_df['Total Sugars (g)'] = menu_data_df['Sugars (g)'] + menu_data_df['Includes (g)']\n",
    "    menu_data_df.drop(columns=['Sugars (g)', 'Includes (g)'], inplace=True)\n",
    "    \n",
    "    # Step 4: Filter rows based on price\n",
    "    menu_data_df = menu_data_df[menu_data_df['price'] >= 1.00]\n",
    "    \n",
    "    # Step 5: Drop discontinued or limited-time items\n",
    "    discontinued_items = [\n",
    "        'Steak and Bacon Grilled Cheese Burrito',\n",
    "        'Strawberry Twists',\n",
    "        'Wild Strawberry Creme Delight Freeze',\n",
    "        'Blue Raspberry Freeze',\n",
    "        'Breakfast Taco Sausage',\n",
    "        'Breakfast Taco Bacon',\n",
    "        'Breakfast Taco Potato',\n",
    "        'Double Berry Freeze',\n",
    "        'Bell Breakfast Box'\n",
    "    ]\n",
    "    menu_data_df = menu_data_df[~menu_data_df['item_name'].isin(discontinued_items)]\n",
    "    \n",
    "    # Step 6: Check and remove duplicates\n",
    "    menu_data_df = menu_data_df.drop_duplicates(subset=['item_name'], keep=\"last\")\n",
    "    \n",
    "    # Step 7: Save cleaned data\n",
    "    if output_csv_path is None:\n",
    "        today = date.today()\n",
    "        output_csv_path = f\"cleaned_taco_bell_menu_items_{today}.csv\"\n",
    "    menu_data_df.to_csv(output_csv_path)\n",
    "    \n",
    "    return menu_data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "clean_taco_bell_menu(\"../data/uncleaned_taco_bell_menu_items2024-08-27.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
